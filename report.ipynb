{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## i. Which insights did you gain from your EDA? \n",
    ">\n",
    ">> * The native naive classifier in the raw dataset was quite ineffective. There were **8213** actual instances of fraud and it was only able to catch 16.\n",
    ">> * There are 8,213 total fraudlent transactions out of 6,362,620 overall transactions.\n",
    ">> * **Only 0.0001291% of all transactions are genuinely fradulent.** \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## ii. How did you determine which columns to drop or keep? If your EDA informed this process, explain which insights you used to determine which columns were not needed. \n",
    ">\n",
    ">> The step, nameOrig, isFlaggedFraud, nameDest columns were removed as they were not needed to determine how best to find and accurately classify fraud. Removing them freed up more computational power to focus on the impactful features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## iii. Which hyperparameter tuning strategy did you use? Grid-search or random-search? Why?\n",
    ">\n",
    ">> Random-Search was implemented due to the immense size of the dataset. A grid search of 6,362,620 rows and 11 columns would have been an unneeded expenditure of computational resources when only 8,213 transactions are genuinely fraudlent."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
